{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "v_DsrKkYODbu",
      "cell_type": "markdown",
      "source": "\n<div style=\"color:#ffffff;\n          font-size:50px;\n          font-style:italic;\n          text-align:left;\n          font-family: 'Lucida Bright';\n          background:#4686C8;\">\n  \t&nbsp; LLM from scratch\n</div>\n<br>   \n<div style=\"\n          font-size:20px;\n          text-align:left;\n          font-family: 'Palatino';\n          \">\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project: LLM from scratch using PyTorch and Python<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Author: George Barrinuevo<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Date: 07/03/2025<br>\n</div>",
      "metadata": {
        "id": "v_DsrKkYODbu"
      }
    },
    {
      "id": "9JV3Tzsa2Vfd",
      "cell_type": "markdown",
      "source": "<br><div style=\"color:#ffffff;\n          font-size:30px;\n          font-style:italic;\n          text-align:left;\n          font-family: 'Lucida Bright';\n          background:#4686C8;\">\n  \t      &nbsp; Project Notes\n</div>\n<div style=\"\n          font-size:16px;\n          text-align:left;\n          font-family: 'Cambria';\">\n    \n<b>Here are my thoughts on this project.</b>\n- The purpose of this project is to create a LLM (Large Language Model) from scratch. It uses PyTorch and Python code.\n- This LLM model uses the Transformer model with Multi-head Attention. The input text is a small text file obtain from a few sources.\n- This model is meant for educational purposes only since the model size and text input size is too small to be of practical use. To convert it to make it usefull requires using a pretrained LLM model where the model and the weights & bias are downloaded which is beyond the scope of this notebook.\n- In preparing for this project I did some self-study. I read a LLM book, studied the basics of PyTorch, reviewed tutorials on Transformers, and etc.\n<br>\n\n<b>Technical Details</b><br><br>\n<u>Input Text</u>\n  - The input text is a plain regular text in a file or downloaded from the internet.\n\n<u>Tokenized Text</u>\n  - The plain text is split in to sentences. Each sentence is split in to words.\n\n<u>Token IDs</u>\n  - Create a vocabulary of text words to token IDs. The words are first sorted alphabetically or by word occurrence frequency. The token IDs are just index numbers that point back to it's correscponding word.\n  - Any punctuations are replaced by an <unknown> token ID.\n\n<u>Token Embedding</u>\n  - Adds a multidimensional matrix used as trainable weights. The number of rows in this matrix is the size of the vocabulary and the number of columns is the embedding dimensional size.\n\n<u>Positional Encoding</u>\n  - This is the position of the word relative to the text sequence using either absolute or relative position methods. This is a matrix which is added to the text embedding matrix.\n\n<u>Input Embeddings</u>\n  - The Token Embeddings and the Positional Encoding matrices are added together to create the Input Embeddings, which are trainable weights with position encoded information.\n\n<u>Sliding Window</u>\n  - The sliding window method is used to create input and target text used to train the model. The sliding window length is the number of consecutive words to consider. The input text is derived from the sliding window that moves from left to right of the text sequence. The target text is the input text with an offset of 1 shifted one word to the right. The training requires the model to predict the next word in the input text which is known by the target text.\n\n<u>Attention</u>\n  - The various types of attention are:\n    - Simplified Attention: This has no trainable weights.\n    - Self-Attention: This has trainable weights.\n    - Causal Attention: This looks only at previous token IDs and does not consider token IDs ahead of the current position. The purpose of this is to not consider future token IDs.\n    - Multi-head attention: This simultaneously computes multiple attentions at a time.\n  - In this notebook, the Multi-head Attention will be used. This method has weights that use Query, Key, and Value to perform it's calculations.\n\n<u>Shortcut</u>\n  - Solves vanishing gradient issue where the gradients become smaller in value when performing back propagation, creating issues while training.\n  - A shortcut allows the gradients to skip a layer. This solves the vanishing gradient issue. The vanishing gradient issue is where the gradients get progressively smaller which creates problems for the learning process.\n\n<u>Layer Normalization</u>\n  - This normalizes the layers output to have a mean of 0 and variance of 1. This will stabilize and accelerate the training process by reducing the impact of different scales and ranges. It then applies a scale and shift which are learnable parameters to insure the output values can represent a wide range of values.\n\n<u>GELU activation</u>\n  - The Gaussian Error Linear Unit (GELU) is an activation function that smooths the output which solves the vanishing gradient issue and improves performance. This is an improvment over the RELU method.\n\n<u>Feed Forward Neural Network</u>\n  - This model extends the embedding dimension to higher dimensions. This allows for improved learning from data.\n  - It consists of a Linear layer, followed by GELU activation, next by another Linear layer. The first layer will expand the dimensions. The second Linear layer will contract the expanded dimension to the original dimension.\n</div>",
      "metadata": {
        "id": "9JV3Tzsa2Vfd"
      }
    },
    {
      "id": "DTqTM2Mm2RvT",
      "cell_type": "code",
      "source": "!pip install torch\n!pip install toktoken\n!pip install matplotlib\n!pip install tensorflow\n!pip install tqdm\n!pip install numpy\n!pip install pandas\n!pip install psutil",
      "metadata": {
        "id": "DTqTM2Mm2RvT"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "92b989e9-da36-4159-b212-799184764dd9",
      "cell_type": "code",
      "source": "import os\nimport tiktoken\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom importlib.metadata import version\nimport urllib.request\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\npkgs = [\"matplotlib\",\n        \"numpy\",\n        \"tiktoken\",\n        \"torch\",\n        \"tensorflow\"\n       ]\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92b989e9-da36-4159-b212-799184764dd9",
        "outputId": "e64f4222-9996-415f-b57d-1e73d6861836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "matplotlib version: 3.10.0\n,numpy version: 2.0.2\n,tiktoken version: 0.9.0\n,torch version: 2.6.0+cu124\n,tensorflow version: 2.18.0\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "qexq5LiEfqHT",
      "cell_type": "code",
      "source": "GPT_CONFIG_124M = {\n    \"vocab_size\": 50257,\n    \"context_length\": 256,\n    \"emb_dim\": 768,\n    \"n_heads\": 12,\n    \"n_layers\": 12,\n    \"drop_rate\": 0.1,\n    \"qkv_bias\": False\n}",
      "metadata": {
        "id": "qexq5LiEfqHT"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "AYJENBmoXURL",
      "cell_type": "code",
      "source": "def create_model(in_cfg):\n    class FeedForward(nn.Module):\n        def __init__(self, cfg):\n            super().__init__()\n            self.layers = nn.Sequential(\n                nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n                nn.GELU(),\n                nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n            )\n\n        def forward(self, x):\n            return self.layers(x)\n\n    class TransformerBlock(nn.Module):\n        def __init__(self, cfg):\n            super().__init__()\n            self.att = MultiHeadAttention(\n                d_in=cfg[\"emb_dim\"],\n                d_out=cfg[\"emb_dim\"],\n                context_length=cfg[\"context_length\"],\n                num_heads=cfg[\"n_heads\"],\n                dropout=cfg[\"drop_rate\"],\n                qkv_bias=cfg[\"qkv_bias\"])\n            self.ff = FeedForward(cfg)\n            self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"])\n            self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"])\n            self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n        def forward(self, x):\n            shortcut = x\n            x = self.norm1(x)\n            x = self.att(x)\n            x = self.drop_shortcut(x)\n            x = x + shortcut\n\n            shortcut = x\n            x = self.norm2(x)\n            x = self.ff(x)\n            x = self.drop_shortcut(x)\n            x = x + shortcut\n\n            return x\n\n    class GPTModel(nn.Module):\n        def __init__(self, cfg):\n            super().__init__()\n            self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n            self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n            self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n            self.trf_blocks = nn.Sequential(\n                *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n            self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n            self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n\n        def forward(self, in_idx):\n            batch_size, seq_len = in_idx.shape\n            tok_embeds = self.tok_emb(in_idx)\n            pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n            x = tok_embeds + pos_embeds\n            x = self.drop_emb(x)\n            x = self.trf_blocks(x)\n            x = self.final_norm(x)\n            logits = self.out_head(x)\n            return logits\n\n    class MultiHeadAttention(nn.Module):\n        def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n            super().__init__()\n            assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n\n            self.d_out = d_out\n            self.num_heads = num_heads\n            self.head_dim = d_out // num_heads\n\n            self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n            self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n            self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n            self.out_proj = nn.Linear(d_out, d_out)\n            self.dropout = nn.Dropout(dropout)\n            self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n        def forward(self, x):\n            b, num_tokens, d_in = x.shape\n\n            keys = self.W_key(x)\n            queries = self.W_query(x)\n            values = self.W_value(x)\n\n            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n            values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n            keys = keys.transpose(1, 2)\n            queries = queries.transpose(1, 2)\n            values = values.transpose(1, 2)\n\n            attn_scores = queries @ keys.transpose(2, 3)\n\n            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n            attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n            attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n            attn_weights = self.dropout(attn_weights)\n\n            context_vec = (attn_weights @ values).transpose(1, 2)\n\n            context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n            context_vec = self.out_proj(context_vec)  # optional projection\n\n            return context_vec\n\n    model = GPTModel(in_cfg)\n    return model\n\nmodel_obj = create_model(GPT_CONFIG_124M)",
      "metadata": {
        "id": "AYJENBmoXURL"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "qA_213AhZ9y1",
      "cell_type": "code",
      "source": "class create_data_handler():\n    def __init__(self):\n        torch.manual_seed(123)\n        self.read_file()\n        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    def read_file(self):\n        global text_data\n\n        file_path = 'input_text.txt'\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            text_data = file.read()\n\n    def text_to_token_ids(self, text, tokenizer):\n        encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n        encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n        return encoded_tensor\n\n    def token_ids_to_text(self, token_ids, tokenizer):\n        flat = token_ids.squeeze(0)\n        return tokenizer.decode(flat.tolist())\n\n    class GPTDataset(Dataset):\n        def __init__(self, txt, tokenizer, max_length, stride):\n            self.input_ids = []\n            self.target_ids = []\n\n            token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n            for i in range(0, len(token_ids) - max_length, stride):\n                input_chunk = token_ids[i:i + max_length]\n                target_chunk = token_ids[i + 1: i + max_length + 1]\n                self.input_ids.append(torch.tensor(input_chunk))\n                self.target_ids.append(torch.tensor(target_chunk))\n\n        def __len__(self):\n            return len(self.input_ids)\n\n        def __getitem__(self, idx):\n            return self.input_ids[idx], self.target_ids[idx]\n\n    def create_dataloader_v1(self, txt, batch_size=4, max_length=256,\n                            stride=128, shuffle=True, drop_last=True, num_workers=0):\n        tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n        dataset = self.GPTDataset(txt, tokenizer, max_length, stride)\n\n        dataloader = DataLoader(\n            dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n\n        return dataloader\n\ndata_obj = create_data_handler()",
      "metadata": {
        "id": "qA_213AhZ9y1"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "0959c855-f860-4358-8b98-bc654f047578",
      "cell_type": "code",
      "source": "class trainer_handler():\n    def __init__(self):\n        self.train_ratio = 0.90\n        self.split_idx = int(self.train_ratio * len(text_data))\n        self.train_data = text_data[:self.split_idx]\n        self.val_data = text_data[self.split_idx:]\n\n        self.train_loader = data_obj.create_dataloader_v1(\n            self.train_data,\n            batch_size=2,\n            max_length=GPT_CONFIG_124M[\"context_length\"],\n            stride=GPT_CONFIG_124M[\"context_length\"],\n            drop_last=True,\n            shuffle=True,\n            num_workers=0\n        )\n\n        self.val_loader = data_obj.create_dataloader_v1(\n            self.val_data,\n            batch_size=2,\n            max_length=GPT_CONFIG_124M[\"context_length\"],\n            stride=GPT_CONFIG_124M[\"context_length\"],\n            drop_last=False,\n            shuffle=False,\n            num_workers=0\n        )\n\n    def calc_loss_batch(self, input_batch, target_batch, model, device):\n        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n        logits = model(input_batch)\n        loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n        return loss\n\n    def calc_loss_loader(self, data_loader, model, device, num_batches=None):\n        total_loss = 0.\n        if len(data_loader) == 0:\n            return float(\"nan\")\n        elif num_batches is None:\n            num_batches = len(data_loader)\n        else:\n            num_batches = min(num_batches, len(data_loader))\n        for i, (input_batch, target_batch) in enumerate(data_loader):\n            if i < num_batches:\n                loss = self.calc_loss_batch(input_batch, target_batch, model, device)\n                total_loss += loss.item()\n            else:\n                break\n        return total_loss / num_batches\n\n    def train_model_simple(self, model, train_loader, val_loader, optimizer, device, num_epochs,\n                          eval_freq, eval_iter, start_context, tokenizer):\n\n        train_losses, val_losses, track_tokens_seen = [], [], []\n        tokens_seen, global_step = 0, -1\n\n        for epoch in range(num_epochs):\n            model.train()\n\n            for input_batch, target_batch in train_loader:\n                optimizer.zero_grad()\n                loss = trainer_obj.calc_loss_batch(input_batch, target_batch, model, device)\n                loss.backward()\n                optimizer.step()\n                tokens_seen += input_batch.numel()\n                global_step += 1\n\n                if global_step % eval_freq == 0:\n                    train_loss, val_loss = self.evaluate_model(\n                        model, train_loader, val_loader, device, eval_iter)\n                    train_losses.append(train_loss)\n                    val_losses.append(val_loss)\n                    track_tokens_seen.append(tokens_seen)\n                    print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                          f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n\n            self.generate_and_print_sample(\n                model, tokenizer, device, start_context\n            )\n\n        return train_losses, val_losses, track_tokens_seen\n\n    def evaluate_model(self, model, train_loader, val_loader, device, eval_iter):\n        model.eval()\n        with torch.no_grad():\n            train_loss = self.calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n            val_loss = self.calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n        model.train()\n        return train_loss, val_loss\n\n    def generate_text_simple(self, model, idx, max_new_tokens, context_size):\n        for _ in range(max_new_tokens):\n\n            idx_cond = idx[:, -context_size:]\n\n            with torch.no_grad():\n                logits = model(idx_cond)\n\n            logits = logits[:, -1, :]\n\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n\n    def generate_and_print_sample(self, model, tokenizer, device, start_context):\n        model.eval()\n        context_size = model.pos_emb.weight.shape[0]\n        encoded = data_obj.text_to_token_ids(start_context, tokenizer).to(device)\n        with torch.no_grad():\n            token_ids = self.generate_text_simple(\n                model=model, idx=encoded,\n                max_new_tokens=50, context_size=context_size\n            )\n        decoded_text = data_obj.token_ids_to_text(token_ids, tokenizer)\n        print(decoded_text.replace(\"\\n\", \" \"))\n        model.train()\n\ntrainer_obj = trainer_handler()",
      "metadata": {
        "id": "0959c855-f860-4358-8b98-bc654f047578"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "3422000b-7aa2-485b-92df-99372cd22311",
      "cell_type": "code",
      "source": "num_epochs = 10\n\ndef do_train():\n    global train_losses, val_losses, tokens_seen\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_obj.to(device)\n    torch.manual_seed(123)\n    model_obj.to(device)\n    optimizer = torch.optim.AdamW(model_obj.parameters(), lr=0.0004, weight_decay=0.1)\n\n    train_losses, val_losses, tokens_seen = trainer_obj.train_model_simple(\n        model_obj, trainer_obj.train_loader, trainer_obj.val_loader, optimizer, device,\n        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n        start_context=\"Every effort moves you\", tokenizer=data_obj.tokenizer\n    )\n\ndo_train()",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3422000b-7aa2-485b-92df-99372cd22311",
        "outputId": "c5a8360a-1034-4747-97bb-cc2f1eb85b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ep 1 (Step 000000): Train loss 0.469, Val loss 7.223\n,Ep 1 (Step 000005): Train loss 0.405, Val loss 7.048\n,Ep 1 (Step 000010): Train loss 0.393, Val loss 7.174\n,Every effort moves you inapplicable to myself any share in the personal emoluments which may be indispensably included in a permanent provision for the executive department; and must accordingly pray that the pecuniary estimates for the station in which I am placed may\n,Ep 2 (Step 000015): Train loss 0.284, Val loss 7.277\n,Ep 2 (Step 000020): Train loss 0.183, Val loss 7.283\n,Ep 2 (Step 000025): Train loss 0.199, Val loss 7.378\n,Every effort moves you in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-\n,Ep 3 (Step 000030): Train loss 0.124, Val loss 7.415\n,Ep 3 (Step 000035): Train loss 0.119, Val loss 7.454\n,Ep 3 (Step 000040): Train loss 0.078, Val loss 7.516\n,Every effort moves you say.  \"I'd rather like to tell you--because I've always suspected you of loathing my work.\"  I made a deprecating gesture, which he negatived with a good-humoured shrug.  \n,Ep 4 (Step 000045): Train loss 0.077, Val loss 7.588\n,Ep 4 (Step 000050): Train loss 0.087, Val loss 7.686\n,Ep 4 (Step 000055): Train loss 0.051, Val loss 7.745\n,Every effort moves you in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-\n,Ep 5 (Step 000060): Train loss 0.042, Val loss 7.761\n,Ep 5 (Step 000065): Train loss 0.036, Val loss 7.745\n,Every effort moves you know not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.  \"My dear, since I've chucked painting people don't say that\n,Ep 6 (Step 000070): Train loss 0.034, Val loss 7.754\n,Ep 6 (Step 000075): Train loss 0.049, Val loss 7.829\n,Ep 6 (Step 000080): Train loss 0.042, Val loss 7.845\n,Every effort moves you inapplicable to myself any share in the personal emoluments which may be indispensably included in a permanent provision for the executive department; and must accordingly pray that the pecuniary estimates for the station in my most egregious mood--\n,Ep 7 (Step 000085): Train loss 0.023, Val loss 7.848\n,Ep 7 (Step 000090): Train loss 0.021, Val loss 7.990\n,Ep 7 (Step 000095): Train loss 0.023, Val loss 7.978\n,Every effort moves you know not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem.\n,Ep 8 (Step 000100): Train loss 0.023, Val loss 7.919\n,Ep 8 (Step 000105): Train loss 0.035, Val loss 7.931\n,Ep 8 (Step 000110): Train loss 0.018, Val loss 7.991\n,Every effort moves you know not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem.\n,Ep 9 (Step 000115): Train loss 0.017, Val loss 8.009\n,Ep 9 (Step 000120): Train loss 0.024, Val loss 8.025\n,Ep 9 (Step 000125): Train loss 0.022, Val loss 8.043\n,Every effort moves you know not to see her absurdity. It was his own absurdity he seemed to be wincing under--his own attitude as an object for garlands and incense.  \"My dear, since I've chucked painting people don't say that\n,Ep 10 (Step 000130): Train loss 0.019, Val loss 8.046\n,Ep 10 (Step 000135): Train loss 0.020, Val loss 8.064\n,Every effort moves you know not to see her absurdity. It was what have we to oppose to be wincing under--his own attitude as an object for garlands and incense.  \"My dear, since I've chucked painting people don't say that\n"
        }
      ],
      "execution_count": 9
    },
    {
      "id": "0WSRu2i0iHJE",
      "cell_type": "code",
      "source": "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    ax2 = ax1.twiny()\n    ax2.plot(tokens_seen, train_losses, alpha=0)\n    ax2.set_xlabel(\"Tokens seen\")\n\n    fig.tight_layout()\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "0WSRu2i0iHJE",
        "outputId": "34e3181e-8b6b-412c-91da-4de1b23b3f9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10
    }
  ]
}